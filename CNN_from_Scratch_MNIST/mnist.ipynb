{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rXSLDqIYGEgp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from random import randint\n",
    "\n",
    "# load MNIST dataset\n",
    "MNIST_data = h5py.File('MNISTdata.hdf5', 'r')\n",
    "# training data\n",
    "x_train = np.float32(MNIST_data['x_train'][:])\n",
    "# label\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0]))\n",
    "# test data,label\n",
    "x_test = np.float32(MNIST_data['x_test'][:])\n",
    "y_test = np.int32(np.array(MNIST_data['y_test'][:,0]))\n",
    "MNIST_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONV_LAYER:\n",
    "    \n",
    "    def __init__(self, dim_ifmap, num_inch, dim_kernel, num_outch, padding):\n",
    "        self.dim_ifmap = dim_ifmap\n",
    "        self.num_inch = num_inch\n",
    "        self.dim_kernel = dim_kernel\n",
    "        self.num_outch = num_outch\n",
    "        self.padding = padding\n",
    "        \n",
    "        # weight\n",
    "        self.kernels = np.random.rand(num_inch, num_outch, dim_kernel, dim_kernel) \\\n",
    "                        / np.sqrt(num_inch*num_outch*dim_kernel*dim_kernel)\n",
    "\n",
    "    def forward(self, ifmap):\n",
    "        \n",
    "        self.dim_ofmap = (self.dim_ifmap - self.dim_kernel + 2*self.padding) + 1 # (W1-F+2P)/1+1\n",
    "        padded_ifmap = np.pad(ifmap, ((0,0),(self.padding, self.padding),(self.padding, self.padding)), 'constant')\n",
    "        ofmap = np.zeros((self.num_outch, self.dim_ofmap, self.dim_ofmap), dtype=float)\n",
    "        for x in range(self.dim_ofmap):\n",
    "            for y in range(self.dim_ofmap):\n",
    "                for k in range(self.num_outch):\n",
    "                    for c in range(self.num_inch):\n",
    "                        for i in range(self.dim_kernel):\n",
    "                            for j in range(self.dim_kernel):\n",
    "                                ofmap[k,x,y] += self.kernels[c, k, i, j]*padded_ifmap[c, x+i, y+j]\n",
    "        return ofmap\n",
    "\n",
    "    def backprop(self, I, dO):\n",
    "\n",
    "        padded_I = np.pad(I,((0,0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "\n",
    "        dK = np.zeros((self.num_inch, self.num_outch, self.dim_kernel, self.dim_kernel), dtype=float)\n",
    "        \n",
    "        for x in range(self.dim_ofmap):\n",
    "            for y in range(self.dim_ofmap):\n",
    "                for k in range(self.num_outch):\n",
    "                    for c in range(self.num_inch):\n",
    "                        for i in range(self.dim_kernel):\n",
    "                            for j in range(self.dim_kernel):\n",
    "                                dK[c, k, i, j] += padded_I[c, x+i, y+j]*dO[k, x, y]\n",
    "\n",
    "        # your job: dI \n",
    "        # dI = dO*K.T\n",
    "        padded_dO =  np.pad(dO,((0,0), (self.padding, self.padding), (self.padding, self.padding)), 'constant')\n",
    "        dI = np.zeros((self.num_inch, self.dim_ifmap, self.dim_ifmap), dtype=float) \n",
    "        for x in range(self.dim_ofmap):\n",
    "            for y in range(self.dim_ofmap):\n",
    "                for k in range(self.num_outch):\n",
    "                    for c in range(self.num_inch):\n",
    "                        for i in range(self.dim_kernel):\n",
    "                            for j in range(self.dim_kernel):\n",
    "                                dI[c, x, y] += padded_dO[c, x+i, y+j]*self.kernels[c,k,j,i]\n",
    "        \n",
    "\n",
    "        return dK, dI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_LAYER:\n",
    "    def __init__(self, num_in, num_out):\n",
    "        self.kernel = np.random.randn(num_in, num_out) / np.sqrt(num_in*num_out)\n",
    "        self.bias = np.random.randn(1, num_out)/np.sqrt(num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z=np.dot(x, self.kernel)+self.bias\n",
    "        return z\n",
    "\n",
    "    def backprop(self, x, dZ2):\n",
    "        dW = np.dot(x.T, dZ2)\n",
    "        dZ1 = np.dot(dZ2, self.kernel.T)\n",
    "        dB = np.sum(dZ2, axis=0, keepdims=True)\n",
    "        return dW, dZ1, dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RELU_LAYER:\n",
    "    def forward(self, x):\n",
    "        return x*(x>0)\n",
    "    def backprop(self,x):\n",
    "        return 1.0*(x>0)\n",
    "\n",
    "def softmax(x):\n",
    "    mrn = np.finfo(x.dtype).max # largest representable number\n",
    "    thr = np.log(mrn / x.size) - 2.0\n",
    "    amx = x.max()\n",
    "    if(amx > thr):\n",
    "        b = np.exp(x - (amx-thr))\n",
    "        return b / (np.exp(thr-amx) + b.sum())\n",
    "    else:\n",
    "        b = np.exp(x)\n",
    "        return b / (1.0 + b.sum())\n",
    "    #return np.exp(x)/np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CROSS_ENTROPY_ERROR:\n",
    "    def forward(self, x, y):\n",
    "        return -1.0*np.sum(np.multiply(np.log(x+0.001e-10), y))\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        return (x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained:  100 / 500 \ttrain accuracy:  0.21 \ttrain cost:  2.2937120571566654\n",
      "Trained:  200 / 500 \ttrain accuracy:  0.4 \ttrain cost:  1.9333881570276228\n",
      "Trained:  300 / 500 \ttrain accuracy:  0.66 \ttrain cost:  1.4994077388066076\n",
      "Trained:  400 / 500 \ttrain accuracy:  0.66 \ttrain cost:  1.3321478792437143\n",
      "Trained:  500 / 500 \ttrain accuracy:  0.7 \ttrain cost:  0.9316994709515752\n",
      "epoch #:  0 \ttest accuracy:  0.75\n",
      "Trained:  100 / 500 \ttrain accuracy:  0.75 \ttrain cost:  0.828859172388919\n",
      "Trained:  200 / 500 \ttrain accuracy:  0.74 \ttrain cost:  0.9464469546958411\n",
      "Trained:  300 / 500 \ttrain accuracy:  0.79 \ttrain cost:  0.6702549839806857\n",
      "Trained:  400 / 500 \ttrain accuracy:  0.78 \ttrain cost:  0.7115635532259899\n",
      "Trained:  500 / 500 \ttrain accuracy:  0.8 \ttrain cost:  0.6772027778434915\n",
      "epoch #:  1 \ttest accuracy:  0.8\n",
      "Trained:  100 / 500 \ttrain accuracy:  0.71 \ttrain cost:  0.8018027033145597\n",
      "Trained:  200 / 500 \ttrain accuracy:  0.81 \ttrain cost:  0.6346302683528304\n",
      "Trained:  300 / 500 \ttrain accuracy:  0.81 \ttrain cost:  0.6018692363960259\n",
      "Trained:  400 / 500 \ttrain accuracy:  0.92 \ttrain cost:  0.3877339181637286\n",
      "Trained:  500 / 500 \ttrain accuracy:  0.89 \ttrain cost:  0.44232282589225913\n",
      "epoch #:  2 \ttest accuracy:  0.83\n"
     ]
    }
   ],
   "source": [
    "# minibatch + conv2 추가 \n",
    "conv1 = CONV_LAYER(dim_ifmap=28, num_inch=1, dim_kernel=3, num_outch=5, padding=1)\n",
    "conv2 = CONV_LAYER(dim_ifmap=28, num_inch=5, dim_kernel=3, num_outch=5, padding=1)\n",
    "relu1 = RELU_LAYER()\n",
    "fc1 = FC_LAYER(28*28*5, 10)\n",
    "cse1 = CROSS_ENTROPY_ERROR()\n",
    "lr = 0.001\n",
    "num_epochs = 3\n",
    "train_iterations = 500\n",
    "test_iterations = 100\n",
    "batch_size=2\n",
    "batch_cnt = int(train_iterations/batch_size)\n",
    "for epoch in range(num_epochs):\n",
    "    total_trained = 0\n",
    "    train_correct = 0\n",
    "    train_cost = 0\n",
    "    #rand_indices = np.random.choice(len(x_train), train_iterations, replace=True)\n",
    "  \n",
    "    for i in range(batch_cnt):\n",
    "        sample = np.random.choice(np.arange(x_train.shape[0]), batch_size)\n",
    "        total_trained += batch_size\n",
    "        # mini batch size가 1일때니까 과제에서 2로 수정\n",
    "        for s in sample:\n",
    "            \n",
    "            mini_x_train = x_train[s].reshape(1, 28, 28)\n",
    "            mini_y_train = y_train[s]\n",
    "            one_hot_y = np.zeros((1,10), dtype=float)\n",
    "            one_hot_y[np.arange(1), mini_y_train]=1.0\n",
    "\n",
    "            # forward propagation\n",
    "            conv1_ofmap = conv1.forward(mini_x_train)\n",
    "            conv2_ofmap = conv2.forward(conv1_ofmap)\n",
    "            relu1_ofmap = relu1.forward(conv2_ofmap)\n",
    "            fc1_out = fc1.forward(relu1_ofmap.reshape(1, 28*28*5))\n",
    "            prob = softmax(fc1_out)\n",
    "            train_cost += cse1.forward(prob, one_hot_y)\n",
    "\n",
    "            #back proppagation\n",
    "            dCSE1 = cse1.backprop(prob, one_hot_y)\n",
    "            dW_FC1, dZ_FC1, dB_FC1 = fc1.backprop(relu1_ofmap.reshape(1, 28*28*5), dCSE1)\n",
    "            dRELU1 = relu1.backprop(conv2_ofmap)\n",
    "            dK_CONV2, dI_CONV2= conv2.backprop(conv1_ofmap, np.multiply(dRELU1, dZ_FC1.reshape(5,28,28)))\n",
    "            #dCONV2=conv2.kernels.T*dRELU1 #??? #dConv2*drelu #dI*K\n",
    "            dK_CONV1, dI_CONV1= conv1.backprop(mini_x_train, np.multiply(dRELU1, dZ_FC1.reshape(5,28,28))) \n",
    "\n",
    "\n",
    "            # weight update\n",
    "            conv1.kernels -= lr*dK_CONV1\n",
    "            conv2.kernels -= lr*dK_CONV2\n",
    "            fc1.kernel -= lr*dW_FC1\n",
    "            fc1.bias -= lr*dB_FC1\n",
    "\n",
    "            train_correct += np.sum(np.equal(np.argmax(prob, axis=1), mini_y_train))\n",
    "\n",
    "        if (total_trained % 100 == 0):\n",
    "            print(\"Trained: \", total_trained, \"/\", train_iterations\\\n",
    "                  , \"\\ttrain accuracy: \", train_correct/100, \"\\ttrain cost: \", train_cost/100)\n",
    "            train_cost = 0\n",
    "            train_correct =0\n",
    "\n",
    "    test_correct = 0\n",
    "    for i in range(test_iterations):\n",
    "        mini_x_test = x_test[i].reshape(1, 28,28)\n",
    "        mini_y_test = y_test[i]\n",
    "\n",
    "        conv1_ofmap = conv1.forward(mini_x_test)\n",
    "        relu1_ofmap = relu1.forward(conv1_ofmap)\n",
    "        fc1_out = fc1.forward(relu1_ofmap.reshape(1, 28*28*5))\n",
    "        prob = softmax(fc1_out)\n",
    "        test_correct += np.sum(np.equal(np.argmax(prob, axis=1), mini_y_test))\n",
    "    print(\"epoch #: \", epoch, \"\\ttest accuracy: \", test_correct/test_iterations)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPK289i3HeT6iNAWN9Vsya",
   "name": "ai_assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
